{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fece011b-6cd2-4a67-a8df-1e81c881a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5d0811-772d-4a56-9e37-5f6edf8ba10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n지정한 chunk_size 이하가 되도록 문자열을 자르는데,\\n기본적으로 [\"\\n\\n\", \"\\n\", \" \", \"\"]와 같은 문자를 이용해 자릅니다. \\n순서대로 가장 먼저 \"\\n\\n\"으로 자르고, 그래도 chunk_size 보다 긴 chunk는 \"\\n\"으로 자르고, \\n그래도 길면 \" \"로 자르는 방식으로 chunk를 만듭니다.\\n여기서 chunk_overlap은 분할된 텍스트 조각들 사이에서 중복으로 포함될 문자수를 정의합니다.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    " 지정한 chunk_size 이하가 되도록 문자열을 자르는데,\n",
    " 기본적으로 [\"\\n\\n\", \"\\n\", \" \", \"\"]와 같은 문자를 이용해 자릅니다. \n",
    " 순서대로 가장 먼저 \"\\n\\n\"으로 자르고, 그래도 chunk_size 보다 긴 chunk는 \"\\n\"으로 자르고, \n",
    " 그래도 길면 \" \"로 자르는 방식으로 chunk를 만듭니다.\n",
    " 여기서 chunk_overlap은 분할된 텍스트 조각들 사이에서 중복으로 포함될 문자수를 정의합니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80172f5b-7e66-4b88-ad3d-b47d4369ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 청크 수: 4\n",
      "\n",
      "청크 1:\n",
      "길이: 10 문자\n",
      "[ABCDEFGHIJ]\n",
      "\n",
      "청크 2:\n",
      "길이: 10 문자\n",
      "[GHIJKLMNOP]\n",
      "\n",
      "청크 3:\n",
      "길이: 10 문자\n",
      "[MNOPQRSTUV]\n",
      "\n",
      "청크 4:\n",
      "길이: 8 문자\n",
      "[STUVWXYZ]\n"
     ]
    }
   ],
   "source": [
    "def basic_example():\n",
    "    # 샘플 텍스트\n",
    "    text = \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # 청크 크기를 10으로 설정합니다.\n",
    "        chunk_size=10,\n",
    "        #청크의 마지막 기준으로 4개 앞까지 포함시킵니다.\n",
    "        chunk_overlap=4,\n",
    "        length_function=len,\n",
    "        separators=[\"\"] #모두 갖도록 합니다.\n",
    "    )\n",
    "    # 텍스트 분할\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"총 청크 수:\", len(chunks))\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n청크 {i}:\")\n",
    "        print(f\"길이: {len(chunk)} 문자\")\n",
    "        print(f\"[{chunk}]\")\n",
    "if __name__ == \"__main__\":\n",
    "    basic_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9b629e-0587-4fa2-9c7f-e8f415b39947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b751ea-86f1-4120-b2f4-db3d03077489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 청크 수: 3\n",
      "\n",
      "청크 1:\n",
      "길이: 18 문자\n",
      "[ABCDEFGHIJKLMNOPQR]\n",
      "\n",
      "청크 2:\n",
      "길이: 17 문자\n",
      "[NOPQRSTUVWXYZ1111]\n",
      "\n",
      "청크 3:\n",
      "길이: 19 문자\n",
      "[VWXYZ11111111111111]\n"
     ]
    }
   ],
   "source": [
    "def basic_example():\n",
    "    # 샘플 텍스트\n",
    "    text = \"\"\"ABCDEFGHIJKLMNOPQRSTUVWXYZ11111111111111\"\"\"\n",
    "    text_splitter = TokenTextSplitter(\n",
    "        chunk_size=10,\n",
    "        chunk_overlap=4,\n",
    "    )\n",
    "    # 텍스트 분할\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"총 청크 수:\", len(chunks))\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n청크 {i}:\")\n",
    "        print(f\"길이: {len(chunk)} 문자\")\n",
    "        print(f\"[{chunk}]\")\n",
    "if __name__ == \"__main__\":\n",
    "    basic_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08bdb0c-1617-4583-b0bb-e70c09a0cb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd92f9d7-91a2-4c35-adc8-4fbddd7eb421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트:\n",
      " \n",
      "    Machine learning is a branch of artificial intelligence. AI systems learn from data.\n",
      "    Deep learning is a subset of machine learning. Neural networks are the foundation of deep learning.\n",
      "    \n",
      "    Climate change is affecting our planet. Global temperatures are rising each year.\n",
      "    Polar ice caps are melting. Sea levels are increasing due to melting ice.\n",
      "    \n",
      "    The solar system contains eight planets. Mars is the fourth planet from the sun.\n",
      "    Jupiter is the largest planet. Saturn is known for its beautiful rings.\n",
      "    \n",
      "\n",
      "==================================================\n",
      "\n",
      "청크 분할 결과:\n",
      "\n",
      "청크 1:\n",
      "\n",
      "    Machine learning is a branch of artificial intelligence. AI systems learn from data. Deep learning is a subset of machine learning. Neural networks are the foundation of deep learning.\n",
      "------------------------------\n",
      "\n",
      "청크 2:\n",
      "Climate change is affecting our planet. Global temperatures are rising each year. Polar ice caps are melting. Sea levels are increasing due to melting ice. The solar system contains eight planets. Mars is the fourth planet from the sun. Jupiter is the largest planet. Saturn is known for its beautiful rings. \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def demonstrate_semantic_chunker():\n",
    "    # BERT 모델을 사용하는 임베딩 설정\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"bert-base-uncased\",  # BERT 모델\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    # SemanticChunker 설정\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    \n",
    "    # 예시 텍스트 (의미적으로 관련된 문장들과 다른 주제의 문장들을 포함)\n",
    "    text = \"\"\"\n",
    "    Machine learning is a branch of artificial intelligence. AI systems learn from data.\n",
    "    Deep learning is a subset of machine learning. Neural networks are the foundation of deep learning.\n",
    "    \n",
    "    Climate change is affecting our planet. Global temperatures are rising each year.\n",
    "    Polar ice caps are melting. Sea levels are increasing due to melting ice.\n",
    "    \n",
    "    The solar system contains eight planets. Mars is the fourth planet from the sun.\n",
    "    Jupiter is the largest planet. Saturn is known for its beautiful rings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 텍스트 분할\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"원본 텍스트:\\n\", text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    print(\"청크 분할 결과:\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n청크 {i}:\")\n",
    "        print(chunk)\n",
    "        print(\"-\"*30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_semantic_chunker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441ecb8-5977-4aca-9193-570795634196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
